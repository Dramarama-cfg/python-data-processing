{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "304de944",
   "metadata": {},
   "source": [
    "#### The primary aim of this code was to automate data processing tasks (for project tracking purpose) that involve:\n",
    "   - Fetching data from a HTML file, that is localy saved on machiene (a cut of project tracking tool with tickets).\n",
    "   - Gathering data from multiple .xlsx files in a directory (files with ticket-related data).\n",
    "   - Cleaning, consolidating and saving final rusults to multiple .xlsx files\n",
    "\n",
    "#### Step-by-step breakdown of what the code does:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f801df",
   "metadata": {},
   "source": [
    "- Initialization: Libraries are imported, warnings are ignored and the maximum number of columns to display in pandas is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd52ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "startTime = time.time()\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e056d",
   "metadata": {},
   "source": [
    "1. **def process_files()**: processes all .xlsx files. For each file, it reads two specific sheets, cleans the data, drops unnecessary columns, consolidates all the processed data and creates a pivot table. \n",
    "    - Each file starts with its own unique ID, and the files always follow a sequence in the folder. If the sequence is broken, it will insert a new line in the place where it was broken, to avoid losing track of a missing project in the future.\n",
    "    \n",
    "        ***More detailed on the above:***\n",
    "        - line 46: retreives unique IDs and set them as index;\n",
    "        - line 49-50: inserting the data from the 'second_column' dataframe into 'proj_details_tab' at position 0, then changes dtype\n",
    "        - line 52-53: It creates a new dataframe, then merges this new dataframe with 'proj_details_tab' on the 'Number_projdetails' column using a right join. This keeps all rows from the new dataframe and matching rows from 'proj_details_tab'.\n",
    "        \n",
    "        \n",
    "    - the final dataframe includes various project-related metrics and is structured to have each row represent a project. The function returns the final consolidated dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42486511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(path):\n",
    "    global proj_details_tab\n",
    "    folder = [file for file in os.listdir(path)]\n",
    "    \n",
    "    consolidated_files = pd.DataFrame()\n",
    "    full_names = pd.DataFrame()\n",
    "\n",
    "    for file in folder:\n",
    "        Per = pd.concat(pd.read_excel('./analyses/'+file, sheet_name=['Per_File']), ignore_index=True)\n",
    "        Per = Per.drop_duplicates(subset=['Language Tag', 'Project Name'], keep='last')\n",
    "        Per['Project Number'] = Per['Project Number'].astype('str')\n",
    "        Total = pd.concat(pd.read_excel('./analyses/'+file, sheet_name=['Total']), ignore_index=True)\n",
    "        Per_new = Per.drop(['Translation Type', 'Project Name', 'Language', 'Name', 'ICE', '100% Matches', '99-95% Matches', '94-85% Matches', '84-75% Matches', '74-0% Matches','Reps', 'Total', 'Adjusted Volume', 'Review Volume', 'Date Due', 'Workgroup'], axis=1)\n",
    "        Total_new = Total.drop(['Total', 'Adjusted Volume', 'Review Volume', 'Language Tag'], axis=1)\n",
    "\n",
    "        f_name = pd.concat(pd.read_excel(path+'\\\\'+file, sheet_name=['Total']), ignore_index=True)\n",
    "        f_name['Project Name'] = file\n",
    "        file_name = pd.concat([f_name], axis=1)\n",
    "        file_name = file_name.drop(['Language Tag', 'Language', 'Translation Type', 'ICE', '100% Matches', '99-95% Matches', '94-85% Matches', '84-75% Matches', '74-0% Matches', 'Reps', 'Total', 'Adjusted Volume', 'Review Volume'], axis=1)\n",
    "        full_names = pd.concat([file_name, full_names], axis=0, ignore_index=True)\n",
    "\n",
    "        Per_new = pd.pivot_table(Per_new, index=['Language Tag'], values='Project Number', aggfunc=lambda x: ', '.join(x))\n",
    "        Per_new['Project Number'] = 'PID' + Per_new['Project Number'] \n",
    "        Per_new.reset_index(inplace=True)\n",
    "\n",
    "        consolidated_sheets = pd.concat([Per_new, Total_new], axis=1)[['Language Tag', 'Project Number', 'Language', 'Translation Type','ICE', '100% Matches', '99-95% Matches', '94-85% Matches', '84-75% Matches', '74-0% Matches', 'Reps']]\n",
    "        consolidated_sheets = consolidated_sheets.iloc[::-1]\n",
    "        consolidated_files = pd.concat([consolidated_sheets, consolidated_files], axis=0, ignore_index=True)\n",
    "        consolidated_files.reset_index(drop=True, inplace=True)\n",
    "        consolidated_files_up = pd.concat([full_names, consolidated_files], axis=1, ignore_index=True)\n",
    "\n",
    "    consolidated_files.reset_index(drop=True, inplace=True)\n",
    "    consolidated_files_up = consolidated_files_up.iloc[::-1]\n",
    "    consolidated_files_up.reset_index(drop=True, inplace=True)\n",
    "    consolidated_files_up.columns = ['File name', 'Language Tag', 'PIDs', 'Language', 'Translation type', 'ICE', '100% Matches', '99-95% Matches', '94-85% Matches', '84-75% Matches', '74-0% Matches', 'Reps']\n",
    "\n",
    "    consolidated_files_up[['Project-HO', 'Project name']] = consolidated_files_up['File name'].str.split('_', 1, expand=True)\n",
    "    consolidated_files_up['Project name'] = consolidated_files_up['Project name'].astype(str).str.rstrip('.xlsx')\n",
    "\n",
    "    consolidated_files_up.drop(['File name', 'Language Tag'], axis=1, inplace=True)\n",
    "    consolidated_files_up = consolidated_files_up[['Project-HO', 'PIDs', 'Project name', 'Language', 'Translation type', 'ICE', '100% Matches', '99-95% Matches', '94-85% Matches', '84-75% Matches', '74-0% Matches', 'Reps']]\n",
    "    consolidated_files_up.columns = ['Project-HO', 'Project Number', 'Project name', 'Language', 'Translation type', 'ICE', '100% Matches', '99-95% Matches', '94-85% Matches', '84-75% Matches', '74-0% Matches', 'Reps']\n",
    "    \n",
    "    proj_details_tab = consolidated_files_up\n",
    "     \n",
    "    proj_details_tab['temp_column2'] = proj_details_tab['Project-HO'].map(lambda x: x.lstrip('Project-HO')).astype('int')\n",
    "    proj_details_tab.set_index(\"temp_column2\")\n",
    "    second_column = proj_details_tab.pop('temp_column2')\n",
    "    proj_details_tab.insert(0, 'Number_projdetails', second_column)\n",
    "    proj_details_tab['Number_projdetails'] = proj_details_tab['Number_projdetails'].astype('int')\n",
    "    \n",
    "    proj_details_tab = proj_details_tab.merge(how='right', on='Number_projdetails', right = pd.DataFrame({'Number_projdetails':np.arange(proj_details_tab.iloc[0]['Number_projdetails'],\n",
    "                                                                                                       proj_details_tab.iloc[-1]['Number_projdetails']+1,1)})).reset_index().drop(['index'], axis=1)\n",
    "    proj_details_tab['Number_projdetails'] = 'Project-HO' + proj_details_tab['Number_projdetails'].astype(str)\n",
    "    proj_details_tab.rename(columns={'Number_projdetails': 'HO Number'}, inplace=True)\n",
    "    proj_details_tab = proj_details_tab.drop('Project-HO', axis=1)\n",
    "    \n",
    "    return proj_details_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88b967b",
   "metadata": {},
   "source": [
    "2. **def fetch_web_data()**: fetches data from a local webpage using selenium WebDriver. It reads different elements from it (based on various xpath criterias), creates a dictionary of fetched data, and returns the data.\n",
    "    - it also checks whether the data was fetched incorrectly. If so, it prints out a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece8468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_web_data():\n",
    "    driver = webdriver.Chrome()\n",
    "    file_path = \"file:///C:/Users/admin/Python/HTML.html\"\n",
    "    driver.get(file_path);\n",
    "    driver.maximize_window();\n",
    "\n",
    "    summary_data = driver.find_elements(\"xpath\", \"//table[@class='tableBorder']/tbody/tr/td/h3/a\") \n",
    "    lang_code = driver.find_elements(\"xpath\", \"//div[@class='table-wrap']/table/tbody/tr[count(*)>3]/td[1]\")\n",
    "    supp_id = driver.find_elements(\"xpath\", \"//div[@class='table-wrap']/table/tbody/tr[count(*)>3]/td[2]/b[font] | //div[@class='table-wrap']/table/tbody/tr[count(*)>3]/td[2]/font[b] | //div[@class='table-wrap']/table/tbody/tr[count(*)>3]/td[position()=2 and not(b[font] or font[b])]\")\n",
    "    dates = driver.find_elements(\"xpath\", \"//div[@class='table-wrap']/table/tbody/tr[count(*)>3]/td[3]\")\n",
    "    dates_green = driver.find_elements(\"xpath\", \"//div[@class='table-wrap']/table/tbody/tr[count(*)>3]/td[3]/b[font] | //div[@class='table-wrap']/table/tbody/tr[count(*)>3]/td[3]/font[b] | //div[@class='table-wrap']/table/tbody/tr[count(*)>3]/td[position()=3 and not(b[font] or font[b])]\")\n",
    "    comments = driver.find_elements(\"xpath\", \"//div[@class='table-wrap']/table/tbody/tr[count(*)>3]/td[4]\")\n",
    "    \n",
    "    # Check if any data fetched is None\n",
    "    if any(item is None for item in [summary_data, lang_code, supp_id, dates, dates_green, comments]):\n",
    "        print(\"Failed to fetch some elements.\")\n",
    "        driver.quit()\n",
    "        return None, None\n",
    "\n",
    "    fetched_data = [{\"Language_Code\": lc.text,\n",
    "                     \"Supplier_ID\": si.text,\n",
    "                     \"Dates\": d.text,\n",
    "                     \"Handback\": d2.text,\n",
    "                     \"Comments\": c.text} for lc, si, d, d2, c in zip(lang_code, supp_id, dates, dates_green, comments)]\n",
    "    \n",
    "    summary_data_array = [{\"Summary\": s.text} for s in summary_data]\n",
    "    \n",
    "    driver.quit()\n",
    "    return fetched_data, summary_data_array\n",
    "\n",
    "def main():\n",
    "    fetched_data, summary_data_array = fetch_web_data()\n",
    "    \n",
    "    if fetched_data is None or summary_data_array is None:\n",
    "        print(\"Data fetching failed.\")\n",
    "        return fetched_data, summary_data_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a33f0f5",
   "metadata": {},
   "source": [
    "3. **def process_fetched_data()**: cleans the fetched data by removing leading and trailing whitespaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0ef3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fetched_data(fetched_data, summary_data_array):\n",
    "    table_data = pd.DataFrame(fetched_data)\n",
    "    table_data = table_data.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    summary_data_df = pd.DataFrame(summary_data_array)\n",
    "    summary_data_df = summary_data_df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    return table_data, summary_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc47d78",
   "metadata": {},
   "source": [
    "4. **def convert_to_datetime()**: converts the 'Dates' column in the table data to a standard datetime format. \n",
    "    - dates were entered manually and don't always stick to a format that pandas can read, some workaround was needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3845543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_datetime(table_data):\n",
    "    # convert to datetime\n",
    "    table_data['Dates'] = (table_data['Dates'].str.replace(' ', '-')\n",
    "                                               .str.replace('-+', '-', regex=True)\n",
    "                                               .str.strip())\n",
    "\n",
    "    table_data_new = table_data['Dates'].str.split('-', expand=True)\n",
    "    table_data_new[2] = table_data_new[2].str[:4]\n",
    "    table_data_new = table_data_new[~table_data_new[0].str.contains('TEP Deadline')]\n",
    "    \n",
    "    table_data_new = table_data_new.rename(columns={\n",
    "        0 : \"Day\", 1 : \"Month\", 2 : \"Year\"\n",
    "    })\n",
    "    \n",
    "    table_data['Dates'] = pd.to_datetime(table_data_new['Day'] + '-' + \n",
    "                                         table_data_new['Month'] + '-' + \n",
    "                                         table_data_new['Year'], \n",
    "                                         errors='coerce').dt.date\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556163eb",
   "metadata": {},
   "source": [
    "5. **def replace_values()**: replaces specific placeholder values in the dataframe with NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae932c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_values(table_data):\n",
    "    # List for replacements \n",
    "    REPL = [\n",
    "        \"REPL_VALUE1\", \"REPL_VALUE2\", \"REPL_VALUE3\"\n",
    "    ]\n",
    "    try:\n",
    "        table_data[[\"Language_Code\", \"Supplier_ID\", \"Dates\", \"Handback\", \"Comments\"]] = table_data[[\"Language_Code\", \"Supplier_ID\", \"Dates\", \"Handback\", \"Comments\"]].replace(REPL, np.nan)\n",
    "    except KeyError as e:\n",
    "        print(f'Column: {e} is missing in the DF')\n",
    "        return None\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e509f23b",
   "metadata": {},
   "source": [
    "6. **def create_group_column() & def extract_and_map_data()**: 'def create_group_column' creates a new column 'group' which increases its value each time it encounters NaN value (in the \"Language_Code\" col), then 'def extract_and_map_data' processes the summary data to extract and map the 'Project-HO' and 'Project_name' columns to the fetched table data using group index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf5f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_group_column(table_data):\n",
    "    # below will create \"group\" column; new number will trigger every time it encounters row with np.nans\n",
    "    table_data[\"group\"] = table_data[\"Language_Code\"].isna().cumsum()\n",
    "    table_data = table_data.sort_values([\"group\", \"Language_Code\"], ascending=True)\n",
    "    return table_data\n",
    "\n",
    "def extract_and_map_data(summary_data_df, table_data):\n",
    "    delimiter_split_df = summary_data_df[\"Summary\"].str.split(\" | \", expand=True)\n",
    "    map_df = delimiter_split_df[[0]].rename(columns={0: \"Project-HO\"})\n",
    "    proj_name = delimiter_split_df[[4]].rename(columns={4: \"Project_name\"}).assign(name=lambda x: x[\"Project_name\"].str.strip().str.replace(\"| \", \"\"))\n",
    "    \n",
    "    # distributes Project-HO and Proj. name withing the groups:\n",
    "    table_data[\"Project-HO\"] = table_data.groupby(\"group\").ngroup().map(map_df[\"Project-HO\"]) #map\n",
    "    table_data[\"Project_name\"] = table_data.groupby(\"group\").ngroup().map(proj_name[\"Project_name\"])\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b640d",
   "metadata": {},
   "source": [
    "7. **def map_supplier_names()**: reads external .xlsx file to create a dictionary mapping between 'Supplier ID' and 'Supplier name'. It then maps the 'Supplier ID' in the table data to the corresponding 'Supplier name'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dc6f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_supplier_names(table_data):\n",
    "    #dict with names:\n",
    "    internal = pd.read_excel(\"../internal_map.xlsx\", sheet_name=\"SuppIDs\") \n",
    "    internal_dict = dict(zip(internal[\"Supplier ID\"], internal[\"Supplier name\"])) \n",
    "    table_data[\"Supplier_Name\"] = table_data[\"Supplier_ID\"].map(internal_dict) \n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc798a2",
   "metadata": {},
   "source": [
    "8. **def filter_and_drop_data()**: cleans the data by replacing a specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_drop_data(table_data):\n",
    "    repl2 = [\n",
    "        \"REPLACE_VALUE\"\n",
    "    ]\n",
    "    table_data = table_data[table_data[\"Language_Code\"] != \"IGNORE\"]\n",
    "    table_data = table_data.dropna(subset=[\"Language_Code\"])\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aa8487",
   "metadata": {},
   "source": [
    "9. **def add_supplier_names()**: adds/replaces values in the 'Supplier_Name' column based on specific conditions in the 'Comments' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa39db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_supplier_names(parsed_cols):\n",
    "    prefixes = (\n",
    "        \"TRIGGER_VALUE\"\n",
    "    )\n",
    "    parsed_cols[\"Supplier_Name\"] = np.where(parsed_cols[\"Comments\"].str.startswith(prefixes)), \n",
    "                                            \"EXTRA_VALUE\", parsed_cols[\"Supplier_Name\"])\n",
    "    parsed_cols[\"Supplier_ID\"] = np.where(parsed_cols[\"Supplier_Name\"] == \"TRIGGER_VALUE2\", \n",
    "                                          \"EXTRA_VALUE\", parsed_cols[\"Supplier_ID\"])\n",
    "    return parsed_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3959b1e",
   "metadata": {},
   "source": [
    "10. **def main()**: all these functions are called in sequence. Cleaned, processed data is then saved into multiple .xlsx files. It also prints out the number of rows in each file, checks if they match, and prints the execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd4ca5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supplier's rows: 83\n",
      "Analysis logs rows: 83\n",
      "Rows match\n",
      "\n",
      "Execution time in seconds: 11.98\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    global parsed_cols\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    startTime = time.time()\n",
    "    \n",
    "    fetched_data, summary_data_array = fetch_web_data()\n",
    "    \n",
    "    if fetched_data is None or summary_data_array is None:\n",
    "        print(\"Data fetching failed\")\n",
    "        return\n",
    "\n",
    "    table_data, summary_data_df = process_fetched_data(fetched_data, summary_data_array)\n",
    "    table_data = convert_to_datetime(table_data)\n",
    "    table_data = replace_values(table_data)\n",
    "    table_data = create_group_column(table_data)\n",
    "    table_data = extract_and_map_data(summary_data_df, table_data)\n",
    "    table_data = map_supplier_names(table_data)\n",
    "    table_data = filter_and_drop_data(table_data)\n",
    "    parsed_cols = table_data[[\"Project-HO\", \"Project_name\", \"Language_Code\", \"Supplier_ID\", \"Supplier_Name\", \"Dates\", \"Handback\", \"Comments\"]]\n",
    "    parsed_cols = add_supplier_names(parsed_cols)\n",
    "    result = process_files('analyses')\n",
    "    \n",
    "\n",
    "    # Save + prints\n",
    "    parsed_cols.to_excel(\"Supplier's_info.xlsx\", sheet_name=\"SUPPLIER_DATA\", index=False)\n",
    "    proj_details_tab.to_excel(\"Analysis_logs.xlsx\", sheet_name=\"PROJECT_DETAILS\", index=False)\n",
    "\n",
    "    num_of_entries_sup = parsed_cols[\"Supplier_ID\"].count()\n",
    "    print(\"Supplier's rows: \" + str(num_of_entries_sup))\n",
    "    \n",
    "    num_of_entries_log = proj_details_tab['HO Number'].count()\n",
    "    print(\"Analysis logs rows: \" + str(num_of_entries_log))\n",
    "    \n",
    "    num_of_nans = proj_details_tab['Project name'].isna().sum()\n",
    "    if num_of_entries_sup == num_of_entries_log:\n",
    "        print('Rows match')\n",
    "    else:\n",
    "        print(\"\\nMissing files from the folder: \" + str(num_of_nans))\n",
    "    \n",
    "    executionTime = round((time.time() - startTime), 2)\n",
    "    print(\"\\nExecution time in seconds: \" + str(executionTime))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a2296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
